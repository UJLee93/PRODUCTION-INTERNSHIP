from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import os

# -------------------- é…ç½® --------------------
BASE_URL = "https://www.pkulaw.com"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
    'Referer': BASE_URL
}

# -------------------- æå–é“¾æ¥å‡½æ•° --------------------
def get_law_detail_info(html_content):
    """
    æå–å½“å‰é¡µé¢çš„æ³•æ¡åç§° + é“¾æ¥
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    laws = []
    for a in soup.select('h4 a[href*="/chl/"]'):
        title = a.get_text(strip=True)
        href = a['href'].split('?')[0]
        full_url = urljoin(BASE_URL, href)
        laws.append((title, full_url))
    return laws

# -------------------- ä¸»æµç¨‹ --------------------
if __name__ == "__main__":
    options = Options()
    # options.add_argument("--headless")  # å¦‚æœä½ ä¸éœ€è¦çœ‹åˆ°æµè§ˆå™¨çª—å£ï¼Œå¯å–æ¶ˆæ³¨é‡Š
    driver = webdriver.Chrome(options=options)

    driver.get(BASE_URL)
    print("ğŸ‘‰ è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ã€ç­›é€‰ã€æ»‘å—éªŒè¯ç­‰æ“ä½œã€‚")

    wait = WebDriverWait(driver, 30)
    visited_urls = set()
    output_file = "æ³•æ¡è¯¦æƒ…é“¾æ¥_åå­—.txt"
    counter = 1  # ç”¨äºç¼–å·

    print("\nâœ… æ¯æ¬¡æ“ä½œå®Œé¡µé¢åï¼ŒæŒ‰ä¸‹å›è½¦ï¼Œæˆ‘å°†æŠ“å–å½“å‰é¡µå¹¶ç­‰å¾…ä¸‹ä¸€è½®ã€‚è¦é€€å‡ºç¨‹åºè¯·ç›´æ¥ Ctrl+C")

    while True:
        input("\nğŸ” æŒ‰å›è½¦æŠ“å–å½“å‰é¡µé¢æ³•æ¡é“¾æ¥ï¼ˆä½ å¯ä»¥å®Œæˆç­›é€‰/è·³é¡µ/éªŒè¯ç­‰åå†æ“ä½œï¼‰...")

        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.accompanying-wrap h4 a")))
        except:
            input("âš ï¸ é¡µé¢åŠ è½½å¤±è´¥æˆ–ç­‰å¾…è¶…æ—¶ï¼Œè¯·å¤„ç†éªŒè¯åå†æŒ‰å›è½¦ç»§ç»­...")

        # è·å–å½“å‰é¡µæºç 
        page_source = driver.page_source
        law_infos = get_law_detail_info(page_source)

        new_count = 0
        with open(output_file, "a", encoding="utf-8") as f:
            for title, url in law_infos:
                if url not in visited_urls:
                    visited_urls.add(url)
                    f.write(f"{counter}ã€{title}ï¼š{url}\n")
                    counter += 1
                    new_count += 1

        print(f"âœ… æœ¬æ¬¡æŠ“å– {new_count} æ¡æ–°æ³•æ¡é“¾æ¥ï¼Œå·²è¿½åŠ åˆ° {output_file}")
